<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8" />
    <title>Deteksi Gestur Isyarat BISINDO</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.22.0/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js"></script>    
  </head>
  <body>
    <h3>TFJS - Test Deteksi</h3>
    <p id="status">Model loading...</p>
    <video id="video-preview" controls autoplay muted width="300"></video>

    <script>
      let model;
      const frameNeeded = 30;
      let extractedFrames = [];
      let lastValidFrame = null;

      // define classes and support set
      const gestureLabels = [
        // A-Z
        "A", "B", "C", "D", "E", "F", "G", "H", "I", "J",
        "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T",
        "U", "V", "W", "X", "Y", "Z",

        // 0-9
        "0", "1", "2", "3", "4", "5", "6", "7", "8", "9",

        // word
        "Aku", "Kamu", "Cinta", "Kenapa", "Tidak", "Kecewa",
        "Mau", "Bicara", "Semangat", "TerimaKasih",
        "Sedih", "Maaf", "Sabar", "Sayang", "Senang"
      ];
      let supportSet = []; // array for support set's keypoints
      let supportLabels = []; // label for support set
      let classNames = []; // class name for support set

      console.log("index.html ready - waiting for messages");

      // load model immediately
      loadModel();
      console.log("DONE - load model");

      // load support set
      loadSupportSet();
      console.log("DONE - load support set");

      // document. or window. depends on each case behavior
      document.addEventListener("message", async (event) => {
        // versi pakai videoUri
        // if (msg?.startsWith('VIDEO_URI:')) {
        //   const videoUri = msg.replace('VIDEO_URI:', '');
        //   document.getElementById('status').innerText = `Received video uri: ${testVidUri}`; // test video uri passed
        //   processVideo(videoUri);
        // }

        console.log("Starting listener... index.html");
        console.log("Entering listener...", event.data);

        try {
          // const msg = event.data;
          const msg = JSON.parse(event.data);
          console.log("üì© Received message from React Native:", msg); // print few
          console.log("Received message:", msg.type);

          // versi dengan base64
          if (msg.type === "VIDEO_BASE64") {
            // const base64 = msg.replace("VIDEO_BASE64:", "");
            document.getElementById("status").innerText = "üé¨ Video received (base64). Try processing...";
            // continue to video processing
            await processVideo(msg.data);
          }
        } catch (error) {
          console.error("‚ùå Error handling base64 video:", error);
          document.getElementById("status").innerText = "‚ùå Failed to process video";
          window.ReactNativeWebView?.postMessage("ERROR:", error.message);
        }
      })

      const hands = new Hands({
        locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`
      });

      // Initialize MediaPipe Hands
      hands.setOptions({
        maxNumHands: 2,
        modelComplexity: 1,
        minDetectionConfidence: 0.5,
        minTrackingConfidence: 0.5
      })

      // Load TensorFlow.js model
      async function loadModel() {
        try {
          model = await tf.loadGraphModel("http://192.168.110.112:8080/model.json");
          document.getElementById("status").innerText = "Model loaded!";
          console.log("‚úÖ Model loaded!");
          window.ReactNativeWebView?.postMessage("MODEL_LOADED");
          console.log("‚úÖ Done - Sent response model loaded");
        } catch (error) {
          document.getElementById("status").innerText = "Failed to load model";
          console.error("‚ùå Failed to load model: ", error);
          window.ReactNativeWebView?.postMessage("ERROR: " + error.message);
        }
      }

      // Load support set
      async function loadSupportSet() {
        // Dalam aplikasi nyata, Anda perlu memuat support set dari dataset eksternal
        // Untuk simulasi, kita akan membuat data dummy (harusnya dari dataset seperti di Python)

        // url supp set keypoints 
        // 1shot: https://drive.google.com/file/d/1iHv5jMT3wwVQ4LhN2U_QNervaPXMVxZx/view?usp=sharing
        // 5shot: https://drive.google.com/file/d/1AAqUtFV-h7bwutK0A-1ZLE1fgtIp9bH7/view?usp=sharing 
        // 10shot: https://drive.google.com/file/d/11cJd7s0j2kWeTiDofIVc0wtpUCrEzRhg/view?usp=sharing 

        // url for fetch
        // 1shot: https://drive.google.com/uc?export=download&id=1iHv5jMT3wwVQ4LhN2U_QNervaPXMVxZx
        // 5shot: https://drive.google.com/uc?export=download&id=1AAqUtFV-h7bwutK0A-1ZLE1fgtIp9bH7
        // 10shot: https://drive.google.com/uc?export=download&id=11cJd7s0j2kWeTiDofIVc0wtpUCrEzRhg

        const jsonUrl = "https://drive.google.com/uc?export=download&id=1iHv5jMT3wwVQ4LhN2U_QNervaPXMVxZx";
        const resp = await fetch(jsonUrl);
        if (!resp.ok) {
          throw new Error(`Failed to fetch support set: ${resp.statusText}`);
        }
        const data = await resp.json();

        
        // const kShot = 1; // 1 sample per class (could be adjusted) - for dummy
        supportSet = [];
        supportLabels = [];
        classNames = [];

        // for dummy
        // for (let i = 0; i < gestureLabels.length; i++) {
        //   const className = gestureLabels[i];
        //   // Simulasi: Buat data dummy untuk support set (30 frame, 84 keypoints)
        //   const dummySample = new Array(30).fill(0).map(() => new Array(84).fill(Math.random())); // Data acak
        //   supportSet.push(dummySample);
        //   supportLabels.push(i);
        //   classNames.push(className);
        // }

        // data validation
        if (!Array.isArray(data) || data.length !== gestureLabels.length) {
          throw new Error(`Support set data length (${data.length}) does not match gestureLabels (${gestureLabels.length})`);
        }

        data.forEach((item, idx) => {
          const label = item.label;
          const keypoints = item.keypoints;

          // label validation
          if (!gestureLabels.includes(label)) {
            console.warn(`Label ${label} not in gestureLabels, skipping...`);
            return;
          }

          // keypoints validation -> must be [30, 84]
          if (!Array.isArray(keypoints) || keypoints.length !== 30 || keypoints[0].length !== 84) {
            throw new Error(`Invalid keypoints shape for class ${label}: Expected [30, 84], got [${keypoints.length}, ${keypoints[0]?.length}]`);
          }

          supportSet.push(keypoints);
          supportLabels.push(idx);
          classNames.push(label);
        })

        console.log(`‚úÖ Loaded support set for ${supportSet.length} classes, ${kShot} samples per class`);
      }

      // Seek to specific time in video
      function seekVideo(video, time) {
        return new Promise((resolve, reject) => {
          video.currentTime = time;
          video.onseeked = () => resolve();
          video.onerror = () => reject(new Error("Video seek failed."));
        })
      }

      // Check if keypoints are valid (not all zeros)
      function isValidKeypoints(keypoints) {
        return keypoints.some((val) => val !== 0);
      }

      // Extract frames with hand landmarks
      async function extractKeyFrames(video) {
        extractedFrames = [];
        lastValidFrame = null;
        const videoWidth = video.videoWidth;
        const videoHeight = video.videoHeight;
        const totalFrames = video.duration * video.fps || 30; // fallback if fps unknown
        const frameInterval = video.duration / frameNeeded; // evenly spaced frames

        console.log(`üìΩÔ∏è Video info: width=${videoWidth}, height=${videoHeight}, duration=${video.duration}s`);

        let processedFrameCount = 0;
        let frameIdx = 0;

        while (processedFrameCount < frameNeeded && frameIdx < totalFrames) {
          const time = frameIdx * frameInterval;
          await seekVideo(video, time);
          const keypoints = await detectHands(video, videoWidth, videoHeight);

          if (keypoints && isValidKeypoints(keypoints)) {
            extractedFrames.push(keypoints);
            lastValidFrame = keypoints.slice(); // copy to avoid reference issue
            processedFrameCount++;
            console.log(`Frame ${frameIdx + 1}: Valid hand data, total valid=${processedFrameCount}`);
          } else {
            console.log(`Frame ${frameIdx + 1}: No valid hand detected`);
          }
          frameIdx++;
        }

        // pad if insufficient frames
        while (extractedFrames.length < frameNeeded) {
          if (lastValidFrame) {
            extractedFrames.push(lastValidFrame.slice());
            console.log("Padding with last valid frame done...");
          } else {
            extractedFrames.push(new Array(84).fill(0));
            console.log("Padding with zeros done...");
          }
        }

        if (extractedFrames.length === 0) {
          throw new Error("No hand detected in any frame");
        }

        console.log(`Extracted ${extractedFrames.length} frames with 84 keypoints each`);
      }

      // Frame Processing - detect hands in current frame
      async function detectHands(video, videoWidth, videoHeight) {
        // Process frame
        const canvas = document.createElement('canvas');
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        const ctx = canvas.getContext('2d');
        ctx.drawImage(video, 0, 0);

        return new Promise((resolve) => {
          hands.onResults((results) => {
            let frameKeypoints = new Array(84).fill(0); // initialize 84 zeros

            if (results.multiHandLandmarks && results.multiHandLandmarks.length > 0) {
              const numDetected = results.multiHandLandmarks.length;
              console.log(`Detected ${numDetected} hands`);
              
              // process up to 2 hands
              for (let i = 0; i < Math.min(numDetected, 2); i++) {
                const handLandmarks = results.multiHandLandmarks[i];
                let keypoints = [];

                for (const lm in handLandmarks) {
                  const x = lm.x * videoWidth; // pixel coordinates (like in Colab)
                  const y = lm.y * videoHeight;
                  keypoints.push(x, y);
                }

                // copy to frameKeypoints (42 values per hand)
                const startIdx = i * 21 * 2;
                frameKeypoints.splice(startIdx, 21 * 2, ...keypoints);
              }
            }
            resolve(frameKeypoints);
          })          
          hands.send({ image : canvas });
        })
      }
      
      // Normalize landmarks (divide by 1080)
      function normalizeLandmarks(keypoints) {
        const maxValue = 1080.0;
        const normalizedKeypoints = keypoints.map((val) => val / maxValue);
        // const normalized = new Array(84).fill(0); // 2 hands * 21 * 2

        console.log("Sample normalized values:", normalizedKeypoints.slice(0, 4).map((v) => v.toFixed(4)));
        return normalizedKeypoints;
      }

      // Prepare input tensor (1, 84, 30)
      function prpInputTensor() {
        // Pad or trunc exactly 30 frames
        let frames = extractedFrames.slice(0, frameNeeded);
        console.log("Original frame shape:", [frames.length, frames[0].length]);

        // normalize keypoints
        frames = frames.map(frame => normalizeLandmarks(frame));

        // transpose frames dari [30, 84] menjadi [84, 30]
        let tensor = tf.tensor2d(frames, [frameNeeded, 84]);
        tensor = tensor.transpose([1, 0]);

        // add batch dimension
        tensor = tensor.expandDims(0); // [1, 84, 30]

        console.log("Final tensor shape:", tensor.shape);
        console.log("Sample normalized values (frame 0, point 0):", frames[0][0].toFixed(4), frames[0][1].toFixed(4));

        // return tf.tensor3d([frames], [1, 84, frameNeeded]);
        return tensor;
      }

      // Compute pairwise distances (Euclidean)
      function computePairwiseDist(embedding1, embedding2) {
        // embedding1: [1, 128], embedding2: [numSupport, 128]
        const diff = embedding1.sub(embedding2); // [numSupport, 128]
        const squaredDiff = diff.square(); // [numSupport, 128]
        const distances = squaredDiff.sum(1).sqrt(); // [numSupport]
        return distances;
      }

      // Run model inference & predict class
      async function runInference(inputTensor, supportTensor, supportLabels, classNames) {
        try {
          // compute embedding for input video
          const videoEmbed = model.execute({ input: inputTensor }); // [1, 128]
          console.log("Video embedding shape:", videoEmbed.shape);

          // compute embedding for support set
          const supportEmbed = model.execute({ input: supportTensor }); // [numSupport, 128]
          console.log("Support embedding shape:", supportEmbed.shape);

          // calculate the Euclidean distance
          const pairwiseDist = computePairwiseDist(videoEmbed, supportEmbed); // [numSupport]
          const distance = pairwiseDist.arraySync();

          // find class with the closest distance
          const nearestIdx = pairwiseDist.argMin().dataSync()[0];
          const predictedLabel = supportLabels[nearestIdx];
          const predictedClass = classNames[predictedLabel];

          // create dictionary of distance for analysis
          const distanceDict = {};
          for (let i = 0; i < distance.length; i++) {
            const label = supportLabels[i];
            const classNm = classNames[label];
            distanceDict[classNm] = distance[i];
          }

          return { predictedClass, distanceDict };

          // old way - usual classifier
          // const output = model.execute({ input: inputTensor });
          // console.log("Raw model output:", output.arraySync()); // Debug output
          // const detection = await output.argMax(-1).data();
          // return detection[0]; // return detected class index
        } catch (error) {
          throw new Error(`Inference failed: ${error.message}`);
        } 
      }

      // Process video from React Native
      async function processVideo(base64Video) {
        console.log("index.html - Start processing video")
        try {
          document.getElementById("status").innerText = "Processing video...";
          console.log("üì• Base64 received, length:", base64Video.length);
          console.log("üì• Base64 preview:", base64Video.slice(0, 30)); // Cek header

          const video = document.createElement('video');
          // video.src = inputVideoUri;
          video.src = `data:video/mp4;base64,${base64Video}`;
          console.log("index.html - Process Video - video.src:", video.src)
          video.crossOrigin = 'anonymous';

          // wait for video metadata
          await new Promise((resolve, reject) => {
            video.onloadedmetadata = () => { 
              console.log("‚úÖ Video metadata loaded: duration =");
              video.currentTime = 0;
              resolve();
            }
            // video.onloadedmetadata = resolve();
            video.onerror = () => {
              console.error("‚ùå Video failed to load, error code:", video.error.code, 'message:', video.error.message);
              reject(new Error(`Failed to load video: ${video.error.message}`));
            }
            video.oncanplay = () => {
              console.log("üì∫ Video can play");
            };
          })

          await hands.initialize();
          console.log("ü§≤ MediaPipe Hands initialized");

          // Extract frames with hand landmarks
          await extractKeyFrames(video);

          // Prepare tensor input (1, 84, 30)
          const inputTensor = prpInputTensor(extractedFrames);

          // Prepare support set tensor
          const supportFrames = supportSet.map(frames => frames.map(frame => normalizeLandmarks(frame)));
          let supportTensor = tf.tensor3d(supportFrames, [supportFrames.length, 30, 84]);
          supportTensor = supportTensor.transpose([0, 2, 1]); // [numSupport, 84, 30]

          // Predict class
          const { predictedClass, distanceDict } = await predictClass(inputTensor, supportTensor, supportLabels, classNames);

          // Run model inference - old way
          // const result = await runInference(inputTensor);

          console.log("üéâ Detection Result:", result);
          console.log("Distances to each class:", distanceDict);

          // Send result back to app (RN)
          const resultMsg = JSON.stringify({
            predictedClass: predictedClass,
            distances: distanceDict
          })
          // window.ReactNativeWebView?.postMessage("RESULT:" + result);
          window.ReactNativeWebView?.postMessage(resultMsg);

          document.getElementById("status").innerText =  `Predicted Class: $predictedClass`;

        } catch (error) {
          console.error("Video processing failed:", error);
          window.ReactNativeWebView?.postMessage("ERROR:" + error.message);
          document.getElementById("status").innerText = "‚ùå Failed to process video";
        }
      }
      
    </script>
  </body>
</html>

<!--
Sip, mantap! Bagian index.html udah siap untuk dipakai dan fleksibel nanti buat nerima input dari React Native-nya üí™

Kalau kamu udah oke, next step kita bisa lanjut ke:

komunikasi antara React Native (WebView) ‚Üî HTML (model predict)

atau ke bagian ekstraksi keypoint + ambil 30 frame

atau bahkan bikin UX-nya supaya jelas alur upload ‚Üí proses ‚Üí tampil output

Kamu mau lanjut ke bagian mana dulu?


Initialize when page loads. when?
window.onload = async () => {
  await loadModel();

  // Listen for messages from app
  window.addEventListener('message', (event) => {
    const testVidUri = event.data;
    document.getElementById('status').innerText = `Dapat video uri: ${testVidUri}`;

    if (event.data && event.data.startsWith('VIDEO_URI:')) {
      const videoUri = event.data.replace('VIDEO_URI:', '');
      processVideo(videoUri);
    }
  })
}

document.AddEventListener old

document.addEventListener("message", async (event) => {
        // const msg = event.data;
        const msg = JSON.parse(event.data);
        console.log("üì© Received message from React Native:", msg.slice(0, 100)); // print few
        console.log("Received message:". msg.type);

        // versi pakai videoUri
        // if (msg?.startsWith('VIDEO_URI:')) {
        //   const videoUri = msg.replace('VIDEO_URI:', '');
        //   document.getElementById('status').innerText = `Received video uri: ${testVidUri}`; // test video uri passed
        //   processVideo(videoUri);
        // }

        // versi dengan base64
        if (msg?.startsWith("VIDEO_BASE64:")) {
          const base64 = msg.replace("VIDEO_BASE64:", "");
          document.getElementById("status").innerText = "üé¨ Video received (base64). Try processing...";

          try {
            // convert base64 to blob -> so we can process the video
            const resp = await fetch(`data:video/mp4;base64,${base64}`);
            const videoBlob = await resp.blob();

            // convert blob to url for preview or processing
            // const blobUrl = URL.createObjectURL(videoBlob);

            // Optional: preview in <video> tag
            const videoElement = document.getElementById("video-preview");
            if (videoElement) {
              videoElement.src = blobUrl;
              videoElement.load();
            }

            // check if model already loaded or not


            // continue to video processing
            processVideo(blobUrl);
          } catch (error) {
            console.error("‚ùå Error handling base64 video:", error);
            document.getElementById("status").innerText = "‚ùå Failed to process video";
            window.ReactNativeWebView?.postMessage("ERROR:", error.message);
          }
          
        }
      })


extractKeyFrames old

async function extractKeyFrames(video) {
        extractedFrames = [];
        lastValidFrame = null;
        const videoWidth = video.videoWidth;
        const videoHeight = video.videoHeight;
        const totalFrames = video.duration * video.fps || 30; // fallback if fps unknown
        const frameInterval = video.duration / 30; // evenly spaced frames

        console.log(`üìΩÔ∏è Video info: width=${videoWidth}, height=${videoHeight}, duration=${video.duration}s`);

        for (let i = 0; i < video.duration * 10 && extractedFrames.length < frameNeeded; i++) {
          await seekVideo(video, i / 10);
          const landmarks = await detectHands(video);

          if (landmarks) {
            lastValidFrame = landmarks; // store for padding
            extractedFrames.push(landmarks);
            console.log(`Frame ${extractedFrames.length}: Valid hand data`);
          }
        }

        if (extractedFrames.length > 0 && extractedFrames[0]) {
          console.log("First frame raw (first 4 values):", extractedFrames[0].slice(0, 4).map(v => v.toFixed(4)));
        } else {
          console.warn("‚ö†Ô∏è No valid frames extracted, cannot log first frame preview.");
        }


        // pad if insufficient frames
        while (extractedFrames.length < frameNeeded && lastValidFrame) {
          extractedFrames.push(lastValidFrame);
          console.log("Padding with last valid frame done...")
        }

        if (extractedFrames.length === 0) {
          throw new Error("No hand detected in any frame");
        }
      }
-->
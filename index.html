<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8" />
    <title>Deteksi Gestur Isyarat BISINDO</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.22.0/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js"></script>    
  </head>
  <body>
    <h3>TFJS - Test Deteksi</h3>
    <p id="status">Model loading...</p>
    <video id="video-preview" controls autoplay muted width="300"></video>

    <script>
      let model;
      const frameNeeded = 30;
      let extractedFrames = [];
      let lastValidFrame = null;

      // define classes and support set
      const gestureLabels = [
        // A-Z
        "A", "B", "C", "D", "E", "F", "G", "H", "I", "J",
        "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T",
        "U", "V", "W", "X", "Y", "Z",

        // 0-9
        "1", "2", "3", "4", "5", "6", "7", "8", "9", "10",

        // word
        "Aku", "Kamu", "Cinta", "Kenapa", "Tidak", "Kecewa",
        "Mau", "Bicara", "Semangat", "TerimaKasih",
        "Sedih", "Maaf", "Sabar", "Sayang", "Senang"
      ];

      // di json: 
      // 1, 2, 3, 4, 5, 6, 7, 8, 9, 10
      //  A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z
      // Aku, Kamu, Cinta, Kenapa, Tidak, Kecewa, Mau, Bicara, Semangat, TerimaKasih, Sedih, Maaf, Sabar, Sayang, Senang


      let supportSet = []; // array for support set's keypoints
      let supportLabels = []; // label for support set
      let classNames = []; // class name for support set

      console.log("index.html ready - waiting for messages");

      // load model immediately
      loadModel();
      console.log("DONE - load model");

      // load support set
      loadSupportSet();
      console.log("DONE - load support set");

      // document. or window. depends on each case behavior
      document.addEventListener("message", async (event) => {
        // versi pakai videoUri
        // if (msg?.startsWith('VIDEO_URI:')) {
        //   const videoUri = msg.replace('VIDEO_URI:', '');
        //   document.getElementById('status').innerText = `Received video uri: ${testVidUri}`; // test video uri passed
        //   processVideo(videoUri);
        // }

        console.log("Starting listener... index.html");
        console.log("Entering listener...", event.data);

        try {
          // const msg = event.data;
          const msg = JSON.parse(event.data);
          console.log("üì© Received message from React Native:", msg); // print few
          console.log("Received message:", msg.type);

          // versi dengan base64
          if (msg.type === "VIDEO_BASE64") {
            // const base64 = msg.replace("VIDEO_BASE64:", "");
            document.getElementById("status").innerText = "üé¨ Video received (base64). Try processing...";
            // continue to video processing
            await processVideo(msg.data);
          }
        } catch (error) {
          console.error("‚ùå Error handling base64 video:", error);
          document.getElementById("status").innerText = "‚ùå Failed to process video";
          window.ReactNativeWebView?.postMessage("ERROR:", error.message);
        }
      })

      const hands = new Hands({
        locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`
      });

      // Initialize MediaPipe Hands
      hands.setOptions({
        maxNumHands: 2,
        modelComplexity: 1,
        minDetectionConfidence: 0.5,
        minTrackingConfidence: 0.5
      })

      // Load TensorFlow.js model
      async function loadModel() {
        try {
          model = await tf.loadGraphModel("http://192.168.93.52:8080/model.json");
          document.getElementById("status").innerText = "Model loaded!";
          console.log("‚úÖ Model loaded!");
          window.ReactNativeWebView?.postMessage("MODEL_LOADED");
          console.log("‚úÖ Done - Sent response model loaded");
        } catch (error) {
          document.getElementById("status").innerText = "Failed to load model";
          console.error("‚ùå Failed to load model: ", error);
          window.ReactNativeWebView?.postMessage("ERROR: " + error.message);
        }
      }

      // Load support set
      async function loadSupportSet() {
        // url supp set keypoints 
        // 1shot: https://drive.google.com/file/d/1iHv5jMT3wwVQ4LhN2U_QNervaPXMVxZx/view?usp=sharing
        // 5shot: https://drive.google.com/file/d/1AAqUtFV-h7bwutK0A-1ZLE1fgtIp9bH7/view?usp=sharing 
        // 10shot: https://drive.google.com/file/d/11cJd7s0j2kWeTiDofIVc0wtpUCrEzRhg/view?usp=sharing 

        // url for fetch
        // 1shot: https://drive.google.com/uc?export=download&id=1iHv5jMT3wwVQ4LhN2U_QNervaPXMVxZx
        // 5shot: https://drive.google.com/uc?export=download&id=1AAqUtFV-h7bwutK0A-1ZLE1fgtIp9bH7
        // 10shot: https://drive.google.com/uc?export=download&id=11cJd7s0j2kWeTiDofIVc0wtpUCrEzRhg

        // no cors - github url
        // 1shot: https://raw.githubusercontent.com/intanparamitha33/air-bisikan-app/refs/heads/main/data/support_set_1.json
        // 5shot: https://raw.githubusercontent.com/intanparamitha33/air-bisikan-app/refs/heads/main/data/support_set_5.json 
        // 10shot: https://raw.githubusercontent.com/intanparamitha33/air-bisikan-app/refs/heads/main/data/support_set_10.json 

        const jsonUrl = "https://raw.githubusercontent.com/intanparamitha33/air-bisikan-app/refs/heads/main/data/support_set_10.json";
        const resp = await fetch(jsonUrl);
        if (!resp.ok) {
          throw new Error(`Error loading support set: ${resp.statusText}`);
        }
        const data = await resp.json();

        
        // const kShot = 1; // 1 sample per class (could be adjusted) - for dummy
        supportSet = [];
        supportLabels = [];
        classNames = [];

        // Check if data structure is array of objects with label and samples
        if (Array.isArray(data)) {
          // count total class to validate
          const uniqueLabels = new Set(data.map(item => item.label));
          console.log(`Found ${uniqueLabels.size} unique labels in support set data.`);

          // process each class
          data.forEach((item, idx) => {
            const label = item.label;
            const samples = item.samples || [item.keypoints]; // support both formats ( 1 or more samples )

            // skip classes not in our gestureLabels
            if (!gestureLabels.includes(label)) {
              console.warn(`Label ${label} not in gestureLabels, skipping...`);
              return;
            }

            // process each sample for this class
            if (Array.isArray(samples)) {
              samples.forEach(keypoints => {
                // keypoints validation - should be [30, 84]
                if (!Array.isArray(keypoints) || keypoints.length !== 30 || keypoints[0].length !== 84) {
                  console.warn(`Invalid keypoints shape for class ${label}: Expected [30, 84], got [${keypoints.length}, ${keypoints[0]?.length}]`);
                  return;
                }

                supportSet.push(keypoints);
                supportLabels.push(gestureLabels.indexOf(label));
                classNames.push(label);
              })
            } else {
              console.warn(`Samples for ${label} is not an array.`);
            }
          })
        } else {
          throw new Error("Invalid support set data format: expected array");
        }
      }

      // Seek to specific time in video
      function seekVideo(video, time) {
        return new Promise((resolve, reject) => {
          video.currentTime = time;
          video.onseeked = () => resolve();
          video.onerror = () => reject(new Error("Video seek failed."));
        })
      }

      // Check if keypoints are valid (not all zeros)
      function isValidKeypoints(keypoints) {
        return keypoints.some((val) => val !== 0);
      }

      // Extract frames with hand landmarks
      async function extractKeyFrames(video) {
        extractedFrames = [];
        lastValidFrame = null;
        const videoWidth = video.videoWidth;
        const videoHeight = video.videoHeight;
        const totalFrames = video.duration * video.fps || 30; // fallback if fps unknown
        const frameInterval = video.duration / frameNeeded; // evenly spaced frames

        console.log(`üìΩÔ∏è Video info: width=${videoWidth}, height=${videoHeight}, duration=${video.duration}s`);

        let processedFrameCount = 0;
        let frameIdx = 0;

        window.ReactNativeWebView?.postMessage("LOG: Starting keyframe extraction...");
        while (processedFrameCount < frameNeeded && frameIdx < totalFrames) {
          const startTime = performance.now();
          const time = frameIdx * frameInterval;
          await seekVideo(video, time);
          const keypoints = await detectHands(video, videoWidth, videoHeight);
          const endTime = performance.now();

          window.ReactNativeWebView?.postMessage("LOG: Frame " + (frameIdx + 1) + " processed in " + (endTime - startTime) + "ms");

          // keyframe selection (optimization) -> push frame with high confidence value
          // del keypoints.confidence if it's not working 
          if (keypoints && isValidKeypoints(keypoints)) {
            extractedFrames.push(keypoints);
            lastValidFrame = keypoints.slice(); // copy to avoid reference issue
            processedFrameCount++;
            console.log(`Frame ${frameIdx + 1}: Valid hand data, total valid=${processedFrameCount}`);
          } else {
            console.log(`Frame ${frameIdx + 1}: No valid hand detected`);
          }
          frameIdx++;
        }

        // pad if insufficient frames
        while (extractedFrames.length < frameNeeded) {
          if (lastValidFrame) {
            extractedFrames.push(lastValidFrame.slice());
            console.log("Padding with last valid frame done...");
          } else {
            extractedFrames.push(new Array(84).fill(0));
            console.log("Padding with zeros done...");
          }
        }

        if (extractedFrames.length === 0) {
          throw new Error("No hand detected in any frame");
        } else if (extractedFrames.length > 0) {
          console.log("First frame keypoints:", extractedFrames[0]);
          window.ReactNativeWebView?.postMessage("LOG: First frame keypoints sample: " + 
            JSON.stringify(extractedFrames[0].slice(0, 10))); // Show first 10 points only
        }

        console.log(`Extracted ${extractedFrames.length} frames with 84 keypoints each`);
        window.ReactNativeWebView?.postMessage("LOG: Extracted " + extractedFrames.length + " frames");
      }

      // Frame Processing - detect hands in current frame
      async function detectHands(video, videoWidth, videoHeight) {
        // Process frame
        const canvas = document.createElement('canvas');
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        const ctx = canvas.getContext('2d');
        ctx.drawImage(video, 0, 0);

        return new Promise((resolve) => {
          hands.onResults((results) => {
            let frameKeypoints = new Array(84).fill(0); // initialize 84 zeros

            if (results.multiHandLandmarks && results.multiHandLandmarks.length > 0) {
              const numDetected = results.multiHandLandmarks.length;
              console.log(`Detected ${numDetected} hands`);
              
              // process up to 2 hands
              for (let i = 0; i < Math.min(numDetected, 2); i++) {
                const handLandmarks = results.multiHandLandmarks[i];
                let keypoints = [];

                for (let j = 0; j < handLandmarks.length; j++) {
                  const lm = handLandmarks[j];
                  const x = lm.x * videoWidth; // pixel coordinates (like in Colab)
                  const y = lm.y * videoHeight;
                  keypoints.push(x, y);
                }

                // copy to frameKeypoints (42 values per hand)
                const startIdx = i * 21 * 2;
                frameKeypoints.splice(startIdx, 21 * 2, ...keypoints);
              }
            }
            resolve(frameKeypoints);
          })          
          hands.send({ image : canvas });
        })
      }
      
      // Normalize landmarks (divide by 1080)
      function normalizeLandmarks(keypoints) {
        const maxValue = 1080.0;
        const normalizedKeypoints = keypoints.map((val) => val / maxValue);
        // const normalized = new Array(84).fill(0); // 2 hands * 21 * 2

        console.log("Sample normalized values:", normalizedKeypoints.slice(0, 4).map((v) => v.toFixed(4)));
        window.ReactNativeWebView?.postMessage("LOG: Normalized keypoints sample: " + 
            JSON.stringify(normalizedKeypoints.slice(0, 10).map(v => v.toFixed(10)))); // Show first 10 points only

        return normalizedKeypoints;
      }

      // Prepare input tensor (1, 84, 30)
      function prpInputTensor() {
        // Pad or trunc exactly 30 frames
        let frames = extractedFrames.slice(0, frameNeeded);
        console.log("Original frame shape:", [frames.length, frames[0].length]);

        // normalize keypoints
        frames = frames.map(frame => normalizeLandmarks(frame));

        // transpose frames dari [30, 84] menjadi [84, 30]
        let tensor = tf.tensor2d(frames, [frameNeeded, 84]);
        tensor = tensor.transpose([1, 0]);

        // add batch dimension
        tensor = tensor.expandDims(0); // [1, 84, 30]

        console.log("Final tensor shape:", tensor.shape); // it seems .shape doesn't work
        window.ReactNativeWebView?.postMessage("LOG: Final tensor shape: [1, 84, 30]");
        console.log("Sample normalized values (frame 0, point 0):", frames[0][0].toFixed(4), frames[0][1].toFixed(4));

        // return tf.tensor3d([frames], [1, 84, frameNeeded]);
        return tensor;
      }

      // Compute pairwise distances (Euclidean)
      function computePairwiseDist(embedding1, embedding2) {
        // embedding1: [1, 128], embedding2: [numSupport, 128]
        const diff = embedding1.sub(embedding2); // [numSupport, 128]
        const squaredDiff = diff.square(); // [numSupport, 128]
        const distances = squaredDiff.sum(1).sqrt(); // [numSupport]
        return distances;
      }

      // Run model inference & predict class
      async function runInference(inputTensor, supportTensor, supportLabels, classNames) {
        try {
          // compute embedding for input video
          const videoEmbed = model.execute({ input: inputTensor }); // [1, 128]
          console.log("Video embedding shape:", videoEmbed.shape);

          // compute embedding for support set
          const supportEmbed = model.execute({ input: supportTensor }); // [numSupport, 128]
          console.log("Support embedding shape:", supportEmbed.shape);

          // calculate the Euclidean distance
          const pairwiseDist = computePairwiseDist(videoEmbed, supportEmbed); // [numSupport]
          const distance = pairwiseDist.arraySync();

          // find class with the closest distance
          // const nearestIdx = pairwiseDist.argMin().dataSync()[0];

          // get all the distances as an array
          // const distArray = Array.from(distance);

          // find the index of the minimum distance
          const nearestIdx = distance.indexOf(Math.min(...distance));

          // get the corresponding label and class name
          const predictedLabel = supportLabels[nearestIdx];
          const predictedClass = classNames[nearestIdx] || "Unknown"; // should use nearestIdx directly, not label || if there's undefined label
          // const predictedClass = classNames[predictedLabel] || "Unknown";

          // create dictionary of distance for analysis
          const distanceDict = {};
          for (let i = 0; i < distance.length; i++) {
            const label = supportLabels[i];
            const classNm = classNames[i]; // use direct index instead of label lookup
            if (distanceDict[classNm] === undefined || distance[i] < distanceDict[classNm]) {
              distanceDict[classNm] = distance[i]; // ensure nearest distance per class 
            }
          }

          // logging values
          const testMinDist = Math.min(...distance);
          window.ReactNativeWebView?.postMessage("LOG: Minimum distance:" + testMinDist);
          window.ReactNativeWebView?.postMessage("LOG: Index of minimum:" + nearestIdx);
          window.ReactNativeWebView?.postMessage("LOG: Predicted class:" + predictedClass);
          window.ReactNativeWebView?.postMessage("LOG: Predicted label:" + predictedLabel);
          window.ReactNativeWebView?.postMessage("LOG: All distances in dictionary:" + distanceDict);

          return { predictedClass, distanceDict };

          // old way - usual classifier
          // const output = model.execute({ input: inputTensor });
          // console.log("Raw model output:", output.arraySync()); // Debug output
          // const detection = await output.argMax(-1).data();
          // return detection[0]; // return detected class index
        } catch (error) {
          throw new Error(`Inference failed: ${error.message}`);
        }
      }

      // Process video from React Native
      async function processVideo(base64Video) {
        console.log("index.html - Start processing video")
        try {
          // function for handling timeout
          const timeoutPromise = new Promise((_, reject) => {
            setTimeout(() => {
              reject(new Error("Process timed out after 30s..."));
            }, 30000); // 30s
          })

          // function for processing video
          const processPromise = new Promise(async (resolve, reject) => {
            try {
              document.getElementById("status").innerText = "Processing video...";
              console.log("üì• Base64 received, length:", base64Video.length);
              console.log("üì• Base64 preview:", base64Video.slice(0, 30)); // Cek header

              const video = document.createElement('video');
              // video.src = inputVideoUri;
              video.src = `data:video/mp4;base64,${base64Video}`;
              console.log("index.html - Process Video - video.src:", video.src)
              video.crossOrigin = 'anonymous';

              // wait for video metadata
              await new Promise((resolve, reject) => {
                video.onloadedmetadata = () => { 
                  console.log("‚úÖ Video metadata loaded: duration =");
                  video.currentTime = 0;
                  resolve();
                }
                // video.onloadedmetadata = resolve();
                video.onerror = () => {
                  console.error("‚ùå Video failed to load, error code:", video.error.code, 'message:', video.error.message);
                  reject(new Error(`Failed to load video: ${video.error.message}`));
                }
                video.oncanplay = () => {
                  console.log("üì∫ Video can play");
                };
              })

              window.ReactNativeWebView?.postMessage("LOG: Initializing MediaPipe Hands...");
              await hands.initialize();
              window.ReactNativeWebView?.postMessage("LOG: ü§≤ MediaPipe Hands initialized");

              // Extract frames with hand landmarks
              window.ReactNativeWebView?.postMessage("LOG: Extracting keyframes input video...");
              await extractKeyFrames(video);
              window.ReactNativeWebView?.postMessage("LOG: Keyframes extracted: " + extractedFrames.length);

              // Prepare tensor input (1, 84, 30)
              window.ReactNativeWebView?.postMessage("LOG: Preparing input tensor...");
              const inputTensor = prpInputTensor(extractedFrames);
              window.ReactNativeWebView?.postMessage("LOG: Input tensor shape: " + inputTensor.shape);

              // Prepare support set tensor
              window.ReactNativeWebView?.postMessage("LOG: Preparing support tensor...");
              const numSupports =  supportSet.length;
              // const supportFrames = supportSet.map(frames => frames.map(frame => normalizeLandmarks(frame)));
              const supportFrames = supportSet;

              // Create tensor with shape [numSupports, 30, 84]
              let supportTensor = tf.tensor3d(supportFrames, [numSupports, 30, 84]);
              supportTensor = supportTensor.transpose([0, 2, 1]); // [numSupports, 84, 30]

              // Predict class
              const { predictedClass, distanceDict } = await runInference(inputTensor, supportTensor, supportLabels, classNames);

              // Run model inference - old way
              // const result = await runInference(inputTensor);

              // Send result back to app (RN)
              const resultMsg = JSON.stringify({
                predictedClass: predictedClass,
                distances: distanceDict
              })
              console.log("Sending to RN:", resultMsg); // Debug
              // window.ReactNativeWebView?.postMessage("RESULT:" + result);
              window.ReactNativeWebView?.postMessage(resultMsg);

              document.getElementById("status").innerText =  `Predicted Class: ${resultMsg.predictedClass}`;
              resolve(resultMsg);
            } catch (error) {
              reject(error);
            }
          })

          // race between process & timeout
          await Promise.race([processPromise, timeoutPromise]);
        } catch (error) {
          // catch err from TO or process
          console.error("‚ùå Error:", error.message);
          window.ReactNativeWebView?.postMessage("LOG: Video processing failed: " + error.message);
          const errorMsg = JSON.stringify({ error: error.message });
          window.ReactNativeWebView?.postMessage(JSON.stringify({ error: error.message }));
          document.getElementById("status").innerText = "‚ùå Failed to process video";
        }
      }
      
    </script>
  </body>
</html>

<!--
Sip, mantap! Bagian index.html udah siap untuk dipakai dan fleksibel nanti buat nerima input dari React Native-nya üí™

Kalau kamu udah oke, next step kita bisa lanjut ke:

komunikasi antara React Native (WebView) ‚Üî HTML (model predict)

atau ke bagian ekstraksi keypoint + ambil 30 frame

atau bahkan bikin UX-nya supaya jelas alur upload ‚Üí proses ‚Üí tampil output

Kamu mau lanjut ke bagian mana dulu?


Initialize when page loads. when?
window.onload = async () => {
  await loadModel();

  // Listen for messages from app
  window.addEventListener('message', (event) => {
    const testVidUri = event.data;
    document.getElementById('status').innerText = `Dapat video uri: ${testVidUri}`;

    if (event.data && event.data.startsWith('VIDEO_URI:')) {
      const videoUri = event.data.replace('VIDEO_URI:', '');
      processVideo(videoUri);
    }
  })
}

document.AddEventListener old

document.addEventListener("message", async (event) => {
        // const msg = event.data;
        const msg = JSON.parse(event.data);
        console.log("üì© Received message from React Native:", msg.slice(0, 100)); // print few
        console.log("Received message:". msg.type);

        // versi pakai videoUri
        // if (msg?.startsWith('VIDEO_URI:')) {
        //   const videoUri = msg.replace('VIDEO_URI:', '');
        //   document.getElementById('status').innerText = `Received video uri: ${testVidUri}`; // test video uri passed
        //   processVideo(videoUri);
        // }

        // versi dengan base64
        if (msg?.startsWith("VIDEO_BASE64:")) {
          const base64 = msg.replace("VIDEO_BASE64:", "");
          document.getElementById("status").innerText = "üé¨ Video received (base64). Try processing...";

          try {
            // convert base64 to blob -> so we can process the video
            const resp = await fetch(`data:video/mp4;base64,${base64}`);
            const videoBlob = await resp.blob();

            // convert blob to url for preview or processing
            // const blobUrl = URL.createObjectURL(videoBlob);

            // Optional: preview in <video> tag
            const videoElement = document.getElementById("video-preview");
            if (videoElement) {
              videoElement.src = blobUrl;
              videoElement.load();
            }

            // check if model already loaded or not


            // continue to video processing
            processVideo(blobUrl);
          } catch (error) {
            console.error("‚ùå Error handling base64 video:", error);
            document.getElementById("status").innerText = "‚ùå Failed to process video";
            window.ReactNativeWebView?.postMessage("ERROR:", error.message);
          }
          
        }
      })


extractKeyFrames old

async function extractKeyFrames(video) {
        extractedFrames = [];
        lastValidFrame = null;
        const videoWidth = video.videoWidth;
        const videoHeight = video.videoHeight;
        const totalFrames = video.duration * video.fps || 30; // fallback if fps unknown
        const frameInterval = video.duration / 30; // evenly spaced frames

        console.log(`üìΩÔ∏è Video info: width=${videoWidth}, height=${videoHeight}, duration=${video.duration}s`);

        for (let i = 0; i < video.duration * 10 && extractedFrames.length < frameNeeded; i++) {
          await seekVideo(video, i / 10);
          const landmarks = await detectHands(video);

          if (landmarks) {
            lastValidFrame = landmarks; // store for padding
            extractedFrames.push(landmarks);
            console.log(`Frame ${extractedFrames.length}: Valid hand data`);
          }
        }

        if (extractedFrames.length > 0 && extractedFrames[0]) {
          console.log("First frame raw (first 4 values):", extractedFrames[0].slice(0, 4).map(v => v.toFixed(4)));
        } else {
          console.warn("‚ö†Ô∏è No valid frames extracted, cannot log first frame preview.");
        }


        // pad if insufficient frames
        while (extractedFrames.length < frameNeeded && lastValidFrame) {
          extractedFrames.push(lastValidFrame);
          console.log("Padding with last valid frame done...")
        }

        if (extractedFrames.length === 0) {
          throw new Error("No hand detected in any frame");
        }
      }

      // data validation - Check if data structure is array of objects with label and samples
        if (!Array.isArray(data) || data.length !== gestureLabels.length) {
          throw new Error(`Support set data length (${data.length}) does not match gestureLabels (${gestureLabels.length})`);
        }

        data.forEach((item, idx) => {
          const label = item.label;
          const keypoints = item.keypoints;

          // label validation
          if (!gestureLabels.includes(label)) {
            console.warn(`Label ${label} not in gestureLabels, skipping...`);
            return;
          }

          // keypoints validation -> must be [30, 84]
          if (!Array.isArray(keypoints) || keypoints.length !== 30 || keypoints[0].length !== 84) {
            throw new Error(`Invalid keypoints shape for class ${label}: Expected [30, 84], got [${keypoints.length}, ${keypoints[0]?.length}]`);
          }

          supportSet.push(keypoints);
          supportLabels.push(idx);
          classNames.push(label);
        })

        console.log(`‚úÖ Loaded support set for ${supportSet.length} classes, 1 samples per class`);

              async function runInference(inputTensor, supportTensor, supportLabels, classNames) {
        try {
          // compute embedding for input video
          const videoEmbed = model.execute({ input: inputTensor }); // [1, 128]
          console.log("Video embedding shape:", videoEmbed.shape);

          // compute embedding for support set
          const supportEmbed = model.execute({ input: supportTensor }); // [numSupport, 128]
          console.log("Support embedding shape:", supportEmbed.shape);

          // calculate the Euclidean distance
          const pairwiseDist = computePairwiseDist(videoEmbed, supportEmbed); // [numSupport]
          const distance = pairwiseDist.arraySync();

          // find class with the closest distance
          // const nearestIdx = pairwiseDist.argMin().dataSync()[0];

          // get all the distances as an array
          const distArray = Array.from(distance);

          // find the index of the minimum distance
          const nearestIdx = distArray.indexOf(Math.min(...distArray));

          // get the corresponding label and class name
          const predictedLabel = supportLabels[nearestIdx];
          const predictedClass = classNames[nearestIdx] || "Unknown"; // should use nearestIdx directly, not label || if there's undefined label
          // const predictedClass = classNames[predictedLabel] || "Unknown";

          // create dictionary of distance for analysis
          const distanceDict = {};
          for (let i = 0; i < distance.length; i++) {
            const label = supportLabels[i];
            const classNm = classNames[i]; // use direct index instead of label lookup
            // const classNm = classNames[label];
            distanceDict[classNm] = distance[i];
          }

          return { predictedClass, distanceDict };

          // old way - usual classifier
          // const output = model.execute({ input: inputTensor });
          // console.log("Raw model output:", output.arraySync()); // Debug output
          // const detection = await output.argMax(-1).data();
          // return detection[0]; // return detected class index
        } catch (error) {
          throw new Error(`Inference failed: ${error.message}`);
        }
-->